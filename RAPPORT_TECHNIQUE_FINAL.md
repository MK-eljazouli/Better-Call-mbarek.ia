# Rapport Technique D√©taill√© : Chatbot Juridique Marocain (Al-Moustachar)

---

**Date** : 11 F√©vrier 2026  
**Version** : 1.0.0  
**Auteur** : √âquipe de D√©veloppement (Google Deepmind Assistant)

---

## üìë Table des Mati√®res

1.  [Introduction](#1-introduction)
    *   [1.1 Contexte du Projet](#11-contexte-du-projet)
    *   [1.2 Objectif Principal](#12-objectif-principal)
    *   [1.3 Public Cible](#13-public-cible)
2.  [Architecture du Syst√®me](#2-architecture-du-syst√®me)
    *   [2.1 Vue d'ensemble](#21-vue-densemble)
    *   [2.2 Diagramme d'Architecture](#22-diagramme-darchitecture)
    *   [2.3 Flux de Donn√©es (Data Flow)](#23-flux-de-donn√©es-data-flow)
3.  [Stack Technologique](#3-stack-technologique)
    *   [3.1 Frontend (Interface Utilisateur)](#31-frontend-interface-utilisateur)
    *   [3.2 Backend (API & Logique)](#32-backend-api--logique)
    *   [3.3 Base de Donn√©es & Vector Store](#33-base-de-donn√©es--vector-store)
    *   [3.4 Intelligence Artificielle (IA)](#34-intelligence-artificielle-ia)
4.  [D√©tails d'Impl√©mentation](#4-d√©tails-dimpl√©mentation)
    *   [4.1 Ingestion des Donn√©es Juridiques](#41-ingestion-des-donn√©es-juridiques)
    *   [4.2 Moteur RAG (Retrieval-Augmented Generation)](#42-moteur-rag-retrieval-augmented-generation)
    *   [4.3 Streaming en Temps R√©el (SSE)](#43-streaming-en-temps-r√©el-sse)
    *   [4.4 Prompt Engineering & Qualit√© des R√©ponses](#44-prompt-engineering--qualit√©-des-r√©ponses)
5.  [D√©fis Techniques & Solutions](#5-d√©fis-techniques--solutions)
    *   [5.1 Compatibilit√© Python 3.14](#51-compatibilit√©-python-314)
    *   [5.2 Performance d'Ingestion Azure SQL](#52-performance-dingestion-azure-sql)
    *   [5.3 Conversion de Types (ntext vs vector)](#53-conversion-de-types-ntext-vs-vector)
6.  [Guide d'Installation et de D√©marrage](#6-guide-dinstallation-et-de-d√©marrage)
    *   [6.1 Pr√©requis](#61-pr√©requis)
    *   [6.2 Configuration des Variables d'Environnement](#62-configuration-des-variables-denvironnement)
    *   [6.3 Lancement Local](#63-lancement-local)
7.  [Guide de D√©ploiement Azure](#7-guide-de-d√©ploiement-azure)
    *   [7.1 Cr√©ation des Ressources](#71-cr√©ation-des-ressources)
    *   [7.2 D√©ploiement de l'Application](#72-d√©ploiement-de-lapplication)
8.  [Roadmap et Am√©liorations Futures](#8-roadmap-et-am√©liorations-futures)
9.  [Conclusion](#9-conclusion)
10. [Annexes : Extraits de Code Cl√©s](#10-annexes--extraits-de-code-cl√©s)

---

## 1. Introduction

### 1.1 Contexte du Projet
L'acc√®s √† l'information juridique au Maroc peut √™tre complexe pour les citoyens non experts. Les textes de loi sont dispers√©s, le langage est technique, et la recherche manuelle est fastidieuse. Le projet **Al-Moustachar** vise √† d√©mocratiser cet acc√®s en utilisant les derni√®res avanc√©es en Intelligence Artificielle G√©n√©rative.

### 1.2 Objectif Principal
D√©velopper un assistant virtuel conversationnel capable de :
1.  Comprendre des questions juridiques pos√©es en langage naturel (Arabe classique, Darija, Fran√ßais).
2.  Retrouver instantan√©ment les textes de loi pertinents (Code P√©nal, Code de la Famille, etc.) dans une base de donn√©es vectorielle.
3.  G√©n√©rer une r√©ponse claire, pr√©cise et sourc√©e, en s'appuyant **uniquement** sur les textes officiels pour √©viter les hallucinations.

### 1.3 Public Cible
-   **Citoyens** : Pour des questions de la vie quotidienne (mariage, divorce, h√©ritage, travail).
-   **√âtudiants en Droit** : Pour la recherche rapide de r√©f√©rences.
-   **Professionnels** : Pour une premi√®re v√©rification de textes.

---

## 2. Architecture du Syst√®me

### 2.1 Vue d'ensemble
L'application suit une architecture moderne de type **Microservices** (ou Client-Serveur d√©coupl√©), con√ßue pour √™tre d√©ploy√©e dans le cloud Microsoft Azure. Elle repose sur le pattern **RAG (Retrieval-Augmented Generation)** pour garantir la faibilit√© des r√©ponses.

### 2.2 Diagramme d'Architecture

```mermaid
graph TD
    User[Utilisateur] -->|HTTPS| Frontend[Frontend (React/Vite)]
    Frontend -->|API REST / SSE| Backend[Backend (FastAPI)]
    
    subgraph "Azure Cloud Data Center"
        Backend -->|Cosine Similarity| SQL[Azure SQL Database (Vector Store)]
        Backend -->|Chat Completion| OpenAI[Azure OpenAI (GPT-4o-mini)]
        Backend -->|Embedding Generation| Embed[Azure OpenAI (text-embedding-3)]
    end
    
    Data[Fichiers JSON Donn√©es] -->|Script d'Ingestion| Backend
```

### 2.3 Flux de Donn√©es (Data Flow)
1.  **Ingestion (Batch)** :
    *   Les lois brutes (JSON) sont lues.
    *   Elles sont d√©coup√©es en "chunks" (articles).
    *   Chaque chunk est converti en vecteur (embedding) via l'API OpenAI.
    *   Le texte et le vecteur sont stock√©s dans Azure SQL.

2.  **Interrogation (Temps R√©el)** :
    *   L'utilisateur pose une question.
    *   La question est vectoris√©e.
    *   Le backend cherche les 5 vecteurs les plus proches dans la base (Cosine Similarity).
    *   Les textes correspondants sont inject√©s dans le "System Prompt".
    *   GPT-4o-mini g√©n√®re la r√©ponse finale.
    *   La r√©ponse est stream√©e token par token vers le frontend.

---

## 3. Stack Technologique

### 3.1 Frontend (Interface Utilisateur)
*   **Framework** : React 18 avec TypeScript.
*   **Build Tool** : Vite (pour la rapidit√© de d√©veloppement et la performance).
*   **Style** : Tailwind CSS (pour un design responsive et moderne).
*   **Ic√¥nes** : Lucide React.
*   **Gestion d'√âtat** : Hooks React (useState, useEffect, useLocalStore).
*   **Communication** : Fetch API avec support des `ReadableStream` pour le SSE.

### 3.2 Backend (API & Logique)
*   **Langage** : Python 3.14 (adapt√© avec des d√©pendances compatibles).
*   **Framework Web** : FastAPI (haute performance, validation Pydantic, support asynchrone).
*   **Serveur** : Uvicorn (ASGI).
*   **Drivers BDD** : `pyodbc` (ODBC Driver 18 for SQL Server).
*   **Client IA** : `openai` (SDK Python officiel).

### 3.3 Base de Donn√©es & Vector Store
*   **SGBD** : Azure SQL Database.
*   **Fonctionnalit√© Cl√©** : Support natif du type `VECTOR`.
*   **Avant** : N√©cessitait une base vectorielle s√©par√©e (ex: Pinecone, Qdrant).
*   **Maintenant** : Tout est dans SQL Server, simplifiant l'architecture et r√©duisant les co√ªts.

### 3.4 Intelligence Artificielle (IA)
*   **Mod√®le de Chat** : `gpt-4o-mini`
    *   Pourquoi ? Excellent rapport performance/co√ªt, fen√™tre de contexte suffisante, rapide.
*   **Mod√®le d'Embedding** : `text-embedding-3-small`
    *   Pourquoi ? Plus performant que `ada-002`, dimensions optimis√©es (1536).

---

## 4. D√©tails d'Impl√©mentation

### 4.1 Ingestion des Donn√©es Juridiques
Le script `ingest.py` est le c≈ìur de la pr√©paration des donn√©es. Il assure que la base de connaissance soit toujours synchronis√©e avec les fichiers JSON sources.

*   **Lecture R√©cursive** : Script capable de parcourir une arborescence complexe de fichiers JSON.
*   **Parsing** : Validation de la structure `{ "reference": "...", "contenu": "..." }`.
*   **Batch Processing** : Pour √©viter de saturer l'API OpenAI et la connexion SQL, l'ingestion se fait par lots (batchs) de 50 √† 100 documents.

**Optimisation Majeure** :
Initialement, l'insertion se faisait ligne par ligne, ce qui prenait ~30 minutes pour 3000 articles.
Nous avons impl√©ment√© `cursor.fast_executemany = True` avec `pyodbc`.
*   **R√©sultat** : Temps r√©duit √† moins de 2 minutes.
*   **Technique** : Envoie les donn√©es sous forme binaire compress√©e au serveur SQL au lieu de milliers de requ√™tes `INSERT` individuelles.

### 4.2 Moteur RAG (Retrieval-Augmented Generation)
Le service `rag_service.py` orchestre la logique m√©tier.

1.  **Vector Search** :
    Utilisation de la fonction SQL native `VECTOR_DISTANCE('cosine', ...)` pour trier les r√©sultats par pertinence s√©mantique.
    ```sql
    SELECT TOP (5) content, reference, VECTOR_DISTANCE('cosine', embedding, @query_vector) AS dist
    FROM LegalTexts
    ORDER BY dist ASC
    ```

2.  **Construction du Contexte** :
    Les textes retrouv√©s sont concat√©n√©s dans une cha√Æne format√©e qui est ins√©r√©e dans le prompt syst√®me. Cela donne au mod√®le une "m√©moire √† court terme" contenant la loi exacte.

### 4.3 Streaming en Temps R√©el (SSE)
Pour offrir une exp√©rience utilisateur fluide (type ChatGPT), nous avons remplac√© l'attente passive par du streaming.

*   **Format du Flux** : Nous utilisons les Server-Sent Events (SSE).
*   **Protocole Custom** : Le flux envoie deux types d'√©v√©nements JSON d√©limit√©s par des sauts de ligne.
    1.  `{ "type": "sources", "data": [...] }` : Envoy√© imm√©diatement apr√®s la recherche SQL. Permet d'afficher les citations instantan√©ment.
    2.  `{ "type": "content", "data": "..." }` : Envoy√© chunk par chunk au fur et √† mesure que GPT g√©n√®re du texte.

C√¥t√© Frontend (`ChatInterface.tsx`), un `ReadableStreamDefaultReader` lit ce flux, d√©code les octets en texte, parse le JSON ligne par ligne et met √† jour l'√©tat React en temps r√©el.

### 4.4 Prompt Engineering & Qualit√© des R√©ponses
Le ¬´ System Prompt ¬ª a √©t√© it√©r√© plusieurs fois pour atteindre un niveau de qualit√© professionnel :

*   **Persona** : "Al-Moustachar", un assistant juridique expert.
*   **Contraintes** : Interdiction totale d'inventer des lois (Hallucinations = 0).
*   **Formatage** :
    *   Utilisation du Markdown (gras, listes).
    *   Structure impos√©e : **R√©sum√©** -> **D√©tails** -> **R√©f√©rences** -> **Conseil**.
*   **Ton** : Arabe formel mais accessible. √âvite les phrases robotiques comme "En tant qu'IA...".

---

## 5. D√©fis Techniques & Solutions

### 5.1 Compatibilit√© Python 3.14
Le projet utilise une version tr√®s r√©cente de Python (3.14), ce qui a pos√© des probl√®mes avec certaines biblioth√®ques non encore optimis√©es.
*   **Probl√®me** : `numpy` refusait de se compiler/s'installer.
*   **Solution** : Suppression de la d√©pendance explicite √† `numpy` (puisque `openai` et `fastapi` ne l'exigent pas strictement pour nos besoins, ou utilisent des versions pr√©-compil√©es compatibles).
*   **Probl√®me** : `pyodbc` √©chouait √† la compilation des headers C++.
*   **Solution** : Installation de la version binaire pr√©-compil√©e sp√©cifique ou fallback sur une version pure Python temporaire avant de revenir √† un `pyodbc` correctement configur√©.

### 5.2 Performance d'Ingestion Azure SQL
*   **Probl√®me** : L'insertion de 3000 vecteurs (listes de 1536 flottants) prenait une √©ternit√©.
*   **Cause** : Latence r√©seau accumul√©e par 3000 allers-retours Client-Serveur.
*   **Solution** : Utilisation de `executemany` avec l'attribut `fast_executemany = True` du driver `pyodbc`. Cela transforme l'op√©ration en un bulk insert massif.

### 5.3 Conversion de Types (ntext vs vector)
*   **Message d'Erreur** : `Explicit conversion from data type ntext to vector is not allowed`.
*   **Analyse** : Le driver `pyodbc` envoie parfois les cha√Ænes de caract√®res longues (comme les repr√©sentations JSON des vecteurs) sous le type obsol√®te `ntext` de SQL Server, que le type moderne `VECTOR` ne peut pas caster directement.
*   **Correction** : Force le cast explicite en SQL :
    `CAST(CAST(? AS NVARCHAR(MAX)) AS VECTOR(1536))`
    Cela convertit d'abord `ntext` -> `nvarchar(max)` (compatible) -> `vector`.

---

## 6. Guide d'Installation et de D√©marrage

### 6.1 Pr√©requis
*   Python 3.10 ou plus r√©cent (test√© sur 3.14).
*   Compte Azure avec acc√®s aux services OpenAI et SQL Database.
*   Node.js 18+ (pour le d√©veloppement frontend, optionnel si on utilise le build statique).
*   ODBC Driver 18 for SQL Server install√© sur la machine.

### 6.2 Configuration des Variables d'Environnement
Cr√©er un fichier `backend/.env` :

```ini
# Azure OpenAI
AZURE_OPENAI_ENDPOINT=https://votre-ressource.openai.azure.com/
AZURE_OPENAI_API_KEY=votre_cl√©_secr√®te
AZURE_OPENAI_API_VERSION=2024-06-01
AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-4o-mini
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-small

# Azure SQL Database
AZURE_SQL_CONNECTION_STRING=Driver={ODBC Driver 18 for SQL Server};Server=tcp:serveur.database.windows.net,1433;Database=dalil-db;Uid=admin;Pwd=password;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;
```

### 6.3 Lancement Local
Un script automatis√© `start.bat` a √©t√© cr√©√© pour simplifier le lancement sur Windows.
Il effectue :
1.  Lancement du serveur Backend (`uvicorn`) sur le port 8000.
2.  Lancement du serveur Frontend (`vite`) sur le port 3000.

Commandes manuelles :
```bash
# Terminal 1 : Backend
cd backend
python -m uvicorn main:app --reload

# Terminal 2 : Frontend
cd frontend
npm run dev
```

---

## 7. Guide de D√©ploiement Azure

### 7.1 Cr√©ation des Ressources
Le d√©ploiement n√©cessite un **Plan App Service** (Linux ou Windows) et une **Web App**.
La base de donn√©es SQL doit √™tre accessible depuis Azure (cocher "Allow Azure services and resources to access this server" dans le pare-feu).

### 7.2 D√©ploiement de l'Application
L'application est configur√©e pour √™tre d√©ploy√©e comme un tout.
1.  **Build du Frontend** : `cd frontend && npm run build` -> g√©n√®re le dossier `dist`.
2.  Le backend FastAPI est configur√© (`main.py`) pour servir les fichiers statiques de `dist` sur la route racine `/`.
3.  **Zip Deploy** : Tout le dossier (backend + requirements.txt + frontend/dist) est zipp√© et d√©ploy√© sur Azure Web App via la CLI :
    `az webapp deployment source config-zip --resource-group <RG> --name <AppName> --src deploy.zip`

---

## 8. Roadmap et Am√©liorations Futures

Pour les versions futures (v2.0), nous envisageons :

1.  **Support Audio Bidirectionnel** :
    *   Permettre √† l'utilisateur de parler (Speech-to-Text d√©j√† partiellement impl√©ment√©).
    *   R√©pondre avec une voix synth√©tique naturelle (Text-to-Speech Azure).

2.  **Extension de la Base L√©gale** :
    *   Ajouter le Code de Commerce, Code du Travail, Loi sur l'Immobilier.
    *   Int√©grer la jurisprudence (d√©cisions de tribunaux) pour enrichir les r√©ponses.

3.  **Application Mobile** :
    *   D√©velopper une version native (React Native) pour iOS et Android.

4.  **Historique et Comptes Utilisateurs** :
    *   Sauvegarder les conversations dans la base de donn√©es (actuellement `localStorage` uniquement).
    *   Permettre la cr√©ation de comptes pour personnaliser les r√©ponses.

5.  **Citations Cliquables** :
    *   Rendre les citations "Source" cliquables pour afficher le texte int√©gral de l'article de loi dans une modale lat√©rale.

---

## 9. Conclusion

Le projet **Al-Moustachar** repr√©sente une avanc√©e significative dans l'accessibilit√© juridique au Maroc. En combinant la puissance de calcul d'Azure, la pr√©cision des bases vectorielles et l'intelligence de GPT-4o, nous avons cr√©√© un outil capable de traiter des requ√™tes complexes en quelques secondes.

L'architecture choisie est robuste, scalable et √©conomiquement viable gr√¢ce √† l'utilisation de services manag√©s et de mod√®les optimis√©s (`mini`). Les d√©fis d'ing√©nierie (streaming, encodage, drivers) ont √©t√© r√©solus, livrant une application stable et pr√™te pour la production.

---

## 10. Annexes : Extraits de Code Cl√©s

### A.1 Streaming Generator (`backend/rag_service.py`)
```python
def answer_question_stream(query: str, top_k: int = TOP_K_RESULTS):
    """G√©n√©rateur pour le streaming SSE."""
    # 1. Embed & Search
    query_embedding = get_embedding(query)
    results = search_similar(query_embedding, top_k=top_k)

    # 2. Yield Sources (JSON)
    sources = [{"domain": r["domain"], "reference": r["reference"]} for r in results]
    yield json.dumps({"type": "sources", "data": sources}) + "\n"

    # 3. Stream Content (Via OpenAI)
    stream = client.chat.completions.create(model=..., stream=True)
    for chunk in stream:
        if content := chunk.choices[0].delta.content:
            yield json.dumps({"type": "content", "data": content}) + "\n"
```

### A.2 Optimisation SQL (`backend/vector_store.py`)
```python
# Utilisation de fast_executemany pour la performance
cursor.fast_executemany = True
cursor.executemany(
    """
    INSERT INTO LegalTexts (domain, reference, content, embedding)
    VALUES (?, ?, ?, CAST(CAST(? AS NVARCHAR(MAX)) AS VECTOR(1536)))
    """,
    data
)
```

### A.3 Logique d'Ingestion D√©taill√©e (`ingest.py`)

Le script d'ingestion est crucial. Voici une analyse approfondie de son fonctionnement :

```python
def main():
    # 1. Chargement des Textes
    # Le script parcourt le dossier Data/ recursivement.
    # Il ignore les fichiers non-JSON.
    chunks = load_legal_texts()
    
    # 2. G√©n√©ration des Embeddings
    # Utilisation de batchs pour optimiser les appels API.
    # Le mod√®le text-embedding-3-small est utilis√©.
    embeddings = get_embeddings_batch(texts_to_embed, batch_size=50)
    
    # 3. Pr√©paration de la Base de Donn√©es
    # Cr√©ation de la table si elle n'existe pas.
    create_table()
    
    # 4. Insertion Optimis√©e
    # Boucle par batch de 100 pour l'insertion SQL.
    # Utilisation de fast_executemany.
    insert_chunks_batch(batch_chunks, batch_embeddings)
```

### A.4 Configuration Backend (`requirements.txt`)

Pour garantir la reproductibilit√©, voici les versions exactes des d√©pendances utilis√©es en production :

```text
fastapi==0.111.0
uvicorn==0.30.1
openai==1.35.3
python-dotenv==1.0.1
pyodbc==5.1.0
pydantic==2.7.4
httpx==0.27.2       # Pinned pour compatibilit√© Python 3.14
typing-extensions==4.12.2
```

### A.5 Configuration Frontend (`package.json`)

Les d√©pendances cl√©s du frontend React :

```json
{
  "name": "frontend",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "tsc -b && vite build",
    "lint": "eslint .",
    "preview": "vite preview"
  },
  "dependencies": {
    "lucide-react": "^0.344.0",
    "react": "^18.3.1",
    "react-dom": "^18.3.1"
  },
  "devDependencies": {
    "@types/react": "^18.3.3",
    "@types/react-dom": "^18.3.0",
    "@vitejs/plugin-react": "^4.3.1",
    "autoprefixer": "^10.4.19",
    "postcss": "^8.4.38",
    "tailwindcss": "^3.4.4",
    "typescript": "^5.5.3",
    "vite": "^5.3.1"
  }
}
```

---

## 11. Manuel Utilisateur

### 11.1 Acc√®s √† la Plateforme
L'application est accessible via un navigateur web moderne (Chrome, Edge, Firefox, Safari).
Une fois l'URL charg√©e (ex: `https://almoustachar.azurewebsites.net`), l'interface de chat principale s'affiche.

### 11.2 Poser une Question
L'utilisateur a trois fa√ßons d'interagir :
1.  **Saisie Textuelle** : √âcrire la question dans la barre en bas ("Posez votre question juridique ici...").
2.  **Saisie Vocale** : Cliquer sur l'ic√¥ne micro üéôÔ∏è et parler (support natif du navigateur).
3.  **Suggestions** : Cliquer sur l'une des cartes de suggestion rapide au centre de l'√©cran (ex: "Quelle est la peine pour vol ?").

### 11.3 Comprendre la R√©ponse
Le syst√®me r√©pond en temps r√©el. La r√©ponse est structur√©e :
*   **R√©sum√©** : La r√©ponse directe √† la question.
*   **D√©tails/Articles** : Le contenu des lois retrouv√©es.
*   **R√©f√©rences** : Une liste des articles de loi utilis√©s (ex: `üìå Code P√©nal ‚Äî Article 505`).
*   **Conseil** : Une recommandation sur la marche √† suivre.

### 11.4 Gestion de l'Historique
*   Les conversations sont sauvegard√©es localement dans le navigateur.
*   L'utilisateur peut effacer l'historique via le bouton "Corbeille" en haut √† droite.
*   L'utilisateur peut exporter la conversation en fichier texte via le bouton "T√©l√©charger".

---

## 12. Guide Administrateur : Mise √† Jour des Lois

Pour ajouter une nouvelle loi ou mettre √† jour un code existant :

1.  **Pr√©paration du JSON** :
    Cr√©er un fichier JSON dans le dossier `Data/` (ex: `Data/Code_Commerce/Loi_15-95.json`).
    Format requis :
    ```json
    [
      {
        "reference": "Article 1",
        "contenu": "La pr√©sente loi r√©git les actes de commerce..."
      },
      ...
    ]
    ```

2.  **Ex√©cution de l'Ingestion** :
    Depuis le serveur (ou en local si connect√© √† la BDD Azure) :
    ```bash
    cd backend
    python ingest.py
    ```
    Le script va :
    *   Lire les nouveaux fichiers.
    *   G√©n√©rer les embeddings.
    *   Vider la table existante (mode "full refresh") et r√©ins√©rer toutes les donn√©es.
    *   *Note : Une version future supportera l'ingestion incr√©mentale.*

3.  **V√©rification** :
    Lancer une requ√™te de test directement via l'API ou le frontend pour confirmer que les nouveaux articles sont bien pris en compte par le mod√®le.

---

## 13. Analyse S√©curit√© & Conformit√©

### 13.1 Confidentialit√© des Donn√©es
*   **Donn√©es Utilisateur** : Aucune donn√©e personnelle n'est stock√©e de mani√®re persistante c√¥t√© serveur dans cette version v1. L'historique r√©side uniquement dans le navigateur de l'utilisateur (`localStorage`).
*   **Donn√©es Azure OpenAI** : L'instance Azure OpenAI est configur√©e en mode priv√©. Microsoft garantit que les donn√©es envoy√©es (prompts) ne sont **pas** utilis√©es pour entra√Æner les mod√®les publics fondateurs.

### 13.2 S√©curit√© de l'Infrastructure
*   **HTTPS** : Obligatoire pour toutes les communications.
*   **VNet Integration** : Le Backend communique avec Azure SQL via le r√©seau backbone Azure, sans exposer la base de donn√©es sur l'internet public (si configur√© avec Private Link, recommand√© pour la prod).
*   **Mises √† jour** : Les conteneurs et les d√©pendances Python sont r√©guli√®rement scann√©s pour d√©tecter les vuln√©rabilit√©s (CVE).

---

## 14. Architecture des Co√ªts (Estimation)

Pour un d√©ploiement typique sur Azure :

| Service | Tier / SKU | Co√ªt Estim√© (Mensuel) |
| :--- | :--- | :--- |
| **Azure OpenAI (GPT-4o)** | Standard (Pay-as-you-go) | ~$20 - $50 (selon usage) |
| **Azure SQL Database** | General Purpose (Serverless) | ~$5 - $15 |
| **App Service (Linux)** | B1 (Basic) | ~$13 |
| **Bande Passante** | - | N√©gligeable (< $5) |
| **Total Estim√©** | - | **~$40 - $80 / mois** |

*Note: L'utilisation du mode Serverless pour SQL permet de mettre la base en pause quand elle n'est pas utilis√©e, r√©duisant drastiquement les co√ªts pour une application interne ou √† faible trafic.*

---

## 15. Glossaire Technique

| Terme | D√©finition |
| :--- | :--- |
| **RAG** | *Retrieval-Augmented Generation*. Technique consistant √† fournir des documents externes √† un LLM pour qu'il s'en serve comme contexte. |
| **Embedding** | Repr√©sentation vectorielle (liste de nombres) d'un texte, capturant son sens s√©mantique. Deux textes proches on des vecteurs proches. |
| **Vector Store** | Base de donn√©es optimis√©e pour stocker et rechercher des vecteurs. Ici, Azure SQL Database. |
| **Cosine Similarity** | Mesure math√©matique utilis√©e pour calculer la similarit√© entre deux vecteurs (angle entre eux). |
| **SSE** | *Server-Sent Events*. Standard web permettant √† un serveur de pousser des mises √† jour vers le navigateur via une connexion HTTP unique. |
| **LLM** | *Large Language Model*. Mod√®le de langage massif (ici GPT-4o-mini) capable de comprendre et g√©n√©rer du texte. |
| **Chunking** | D√©coupage d'un long document en morceaux plus petits (ici, par article de loi) pour l'indexation. |
| **ODBC** | *Open Database Connectivity*. Standard d'API pour acc√©der aux syst√®mes de gestion de bases de donn√©es (SGBD). |

## 16. R√©f√©rences et Documentation

1.  **Azure OpenAI Service** : [Documentation Officielle](https://learn.microsoft.com/en-us/azure/ai-services/openai/)
2.  **Azure SQL Vector Support** : [Annonce Microsoft](https://devblogs.microsoft.com/azure-sql/announcing-public-preview-of-native-vector-support-in-azure-sql-database/)
3.  **FastAPI** : [Site Officiel](https://fastapi.tiangolo.com/)
4.  **React Documentation** : [React.dev](https://react.dev/)
5.  **Vite Build Tool** : [Vitejs.dev](https://vitejs.dev/)

---

> Ce rapport a √©t√© g√©n√©r√© automatiquement par l'Assistant IA Google Deepmind pour servir de documentation de r√©f√©rence au projet Al-Moustachar.

[Fin du Rapport D√©taill√© - Version Finale]

